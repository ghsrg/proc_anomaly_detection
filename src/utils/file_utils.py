import pandas as pd
import numpy as np
import h5py
import networkx as nx
from src.utils.file_utils_l import make_dir, join_path, is_file_exist
from src.utils.logger import get_logger
from src.config.config import RAW_PATH, REGISTER_PATH
import gzip
import json
import torch
from pathlib import Path
import os
import re
logger = get_logger(__name__)


def save_to_parquet(df: pd.DataFrame, file_name: str):
    """
    –ó–±–µ—Ä—ñ–≥–∞—î —Å–∏—Ä—ñ –¥–∞–Ω—ñ —É —Ñ–æ—Ä–º–∞—Ç—ñ Parquet.
    :param df: DataFrame –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è.
    :param file_name: –ù–∞–∑–≤–∞ —Ñ–∞–π–ª—É –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è.
    """
    raw_data_path = join_path([RAW_PATH, f"{file_name}.parquet"])
    df.to_parquet(raw_data_path, engine="pyarrow", index=False)
    logger.info(f"–î–∞–Ω—ñ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É {raw_data_path}")



def aggregate_statistics(directory_path):
    all_results = []

    for filename in os.listdir(directory_path):
        if filename.endswith("statistics.xlsx"):
            full_path = os.path.join(directory_path, filename)

            # –í–∏–ø—Ä–∞–≤–ª–µ–Ω–∞ —Ä–µ–≥—É–ª—è—Ä–∫–∞
            match = re.match(r"(.+?)(?:_pr)?_(logs|bpmn)_seed(\d+)_statistics\.xlsx", filename)
            if match:
                architecture, data_type, seed = match.groups()

                try:
                    df = pd.read_excel(full_path)

                    if not df.empty:
                        print(f' Aggregate statistics: {filename}')
                        last_row = df.iloc[-1]  # –±–µ—Ä–µ–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ–π —Ä—è–¥–æ–∫
                        result = {
                            "architecture": architecture,
                            "data_type": data_type,
                            "seed": int(seed),
                            "epoch": last_row['epochs'],
                            "train_loss": last_row['train_loss'],
                            "spend_time": last_row['spend_time'],
                            "val_accuracy": last_row['val_accuracy'],
                            "val_top_k_accuracy": last_row['val_top_k_accuracy'],
                            "val_out_of_scope_rate": last_row['val_out_of_scope_rate']
                        }
                        all_results.append(result)
                except Exception as e:
                    print(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ–±—Ä–æ–±—Ü—ñ {filename}: {e}")

    # –û–±'—î–¥–Ω–∞—Ç–∏ –≤—Å–µ –≤ –æ–¥–∏–Ω DataFrame
    final_df = pd.DataFrame(all_results)
    return final_df

def load_and_aggregate_confusion_matrices(
    folder_path,
    data_type_filter="bpmn",
    reduction="avg",  # –∞–±–æ 'sum'
    normalize=True
):
    """
    –ó—á–∏—Ç—É—î –≤—Å—ñ .xlsx confusion –º–∞—Ç—Ä–∏—Ü—ñ –∑ –≤–∫–∞–∑–∞–Ω–æ—ó –ø–∞–ø–∫–∏, —Ñ—ñ–ª—å—Ç—Ä—É—î –∑–∞ —Ç–∏–ø–æ–º (logs/bpmn),
    —ñ –ø–æ–≤–µ—Ä—Ç–∞—î –æ–¥–Ω—É –∑–≤–µ–¥–µ–Ω—É –º–∞—Ç—Ä–∏—Ü—é.

    –ü–∞—Ä–∞–º–µ—Ç—Ä–∏:
    - folder_path: —à–ª—è—Ö –¥–æ –ø–∞–ø–∫–∏ –∑ .xlsx —Ñ–∞–π–ª–∞–º–∏ –º–∞—Ç—Ä–∏—Ü—å
    - data_type_filter: 'logs' –∞–±–æ 'bpmn'
    - reduction: 'avg' –∞–±–æ 'sum'
    - normalize: —á–∏ –Ω–æ—Ä–º–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –ø–æ —Ä—è–¥–∫–∞—Ö (True/False)

    –ü–æ–≤–µ—Ä—Ç–∞—î:
    - aggregated_df: —Ñ—ñ–Ω–∞–ª—å–Ω–∞ –º–∞—Ç—Ä–∏—Ü—è pandas.DataFrame
    """
    matrices = []

    for filename in os.listdir(folder_path):
        if filename.endswith("CM.png_full.xlsx") and data_type_filter in filename:
            full_path = os.path.join(folder_path, filename)
            try:
                print(f' Aggregate CMs: {filename}')
                df = pd.read_excel(full_path, index_col=0)
                matrices.append(df)
            except Exception as e:
                print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑—á–∏—Ç—É–≤–∞–Ω–Ω—ñ {filename}: {e}")

    if not matrices:
        print("‚ö†Ô∏è –ù–µ–º–∞—î –º–∞—Ç—Ä–∏—Ü—å –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ü—ñ—ó.")
        return None

    # –í–∏—Ä—ñ–≤–Ω—è—Ç–∏ –ø–æ—Ä—è–¥–æ–∫ —ñ–Ω–¥–µ–∫—Å—ñ–≤/—Å—Ç–æ–≤–ø—Ü—ñ–≤
    all_labels = sorted(set().union(*[m.index for m in matrices]))
    matrices = [m.reindex(index=all_labels, columns=all_labels, fill_value=0) for m in matrices]

    # –ê–≥—Ä–µ–≥–∞—Ü—ñ—è
    if reduction == "sum":
        aggregated_df = sum(matrices)
    else:
        aggregated_df = sum(matrices) / len(matrices)

    # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –ø–æ —Ä—è–¥–∫–∞—Ö
    if normalize:
        aggregated_df = aggregated_df.div(aggregated_df.sum(axis=1), axis=0).fillna(0)

    return aggregated_df

def combine_activity_stat_files_clear(directory):
    all_dfs = []
    for filename in os.listdir(directory):
        if filename.endswith("_accuracy_stat.xlsx"):
            print(f' Aggregate distributions: {filename}')
            df = pd.read_excel(os.path.join(directory, filename))
            all_dfs.append(df)
    combined_df = pd.concat(all_dfs, ignore_index=True)
    return combined_df


def combine_activity_stat_files(directory):
    all_dfs = []
    for filename in os.listdir(directory):
        if filename.endswith("_accuracy_stat.xlsx"):
            print(f' Aggregate distributions: {filename}')
            df = pd.read_excel(os.path.join(directory, filename))
            all_dfs.append(df)

    combined_df = pd.concat(all_dfs, ignore_index=True)

    # üîß –ú–Ω–æ–∂–∏–º–æ train_count –Ω–∞ 2 —Ç—ñ–ª—å–∫–∏ –¥–ª—è logs
    if "mode" in combined_df.columns and "train_count" in combined_df.columns:
        combined_df.loc[combined_df["mode"] == "logs", "train_count"] *= 2

    return combined_df


def save_activity_stats_to_excel(activity_stats, architecture, mode, seed, file_path):
    """
    –ó–±–µ—Ä—ñ–≥–∞—î —Å–ª–æ–≤–Ω–∏–∫ activity_train_vs_val_accuracy —É Excel —ñ–∑ –¥–æ–¥–∞—Ç–∫–æ–≤–∏–º–∏ –∫–æ–ª–æ–Ω–∫–∞–º–∏.

    Parameters:
    - activity_stats: dict (node_id -> {"train_count": ..., "val_accuracy": ...})
    - architecture: –Ω–∞–∑–≤–∞ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏ (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥: "GAT")
    - mode: —Ä–µ–∂–∏–º (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥: "bpmn" –∞–±–æ "logs")
    - file_path: —à–ª—è—Ö –¥–æ Excel —Ñ–∞–π–ª—É (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥: "activity_distributions.xlsx")
    """
    df = pd.DataFrame.from_dict(activity_stats, orient="index")
    df.reset_index(inplace=True)
    df.rename(columns={"index": "node_id"}, inplace=True)
    df["architecture"] = architecture
    df["mode"] = mode
    df["seed"] = seed
    df.to_excel(file_path, index=False)
def aggregate_metric_over_epochs(directory_path, metric_name):
    """
    –ó–±–∏—Ä–∞—î –∑–∞–¥–∞–Ω—É –º–µ—Ç—Ä–∏–∫—É –ø–æ –µ–ø–æ—Ö–∞—Ö –¥–ª—è –∫–æ–∂–Ω–æ—ó –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏, seed —ñ —Ç–∏–ø—É –¥–∞–Ω–∏—Ö (logs/bpmn).

    –ü–æ–≤–µ—Ä—Ç–∞—î DataFrame —É —Ñ–æ—Ä–º–∞—Ç—ñ:
    | architecture | data_type | seed | epoch | metric_value |
    """
    all_records = []

    for filename in os.listdir(directory_path):
        if filename.endswith("statistics.xlsx"):
            full_path = os.path.join(directory_path, filename)

            match = re.match(r"(.+?)(?:_pr)?_(logs|bpmn)_seed(\d+)_statistics\.xlsx", filename)
            if match:
                architecture, data_type, seed = match.groups()

                try:
                    df = pd.read_excel(full_path)

                    if not df.empty and metric_name in df.columns:
                        for idx, row in df.iterrows():
                            record = {
                                "architecture": architecture,
                                "data_type": data_type,
                                "seed": int(seed),
                                "epoch": int(row['epochs']),
                                "metric_value": row[metric_name]
                            }
                            all_records.append(record)
                except Exception as e:
                    print(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ–±—Ä–æ–±—Ü—ñ {filename}: {e}")

    return pd.DataFrame(all_records)

def save_aggregated_statistics(df, output_path):
    """
    –ó–±–µ—Ä—ñ–≥–∞—î –∞–≥—Ä–µ–≥–æ–≤–∞–Ω—É —Ç–∞–±–ª–∏—Ü—é —É —Ñ–æ—Ä–º–∞—Ç—ñ Excel –∞–±–æ CSV.

    :param df: DataFrame –∑ –∞–≥—Ä–µ–≥–æ–≤–∞–Ω–∏–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    :param output_path: –®–ª—è—Ö –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ñ–∞–π–ª—É (–∑ —Ä–æ–∑—à–∏—Ä–µ–Ω–Ω—è–º .xlsx –∞–±–æ .csv)
    """
    if output_path.endswith(".xlsx"):
        df.to_excel(output_path, index=False)
        print(f"–ê–≥—Ä–µ–≥–æ–≤–∞–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–∞ —É {output_path}")
    elif output_path.endswith(".csv"):
        df.to_csv(output_path, index=False)
        print(f"–ê–≥—Ä–µ–≥–æ–≤–∞–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–∞ —É {output_path}")
    else:
        raise ValueError("–§–∞–π–ª –º–∞—î –∑–∞–∫—ñ–Ω—á—É–≤–∞—Ç–∏—Å—å –Ω–∞ '.xlsx' –∞–±–æ '.csv'")


def read_from_parquet(file_name: str, columns=None) -> pd.DataFrame:
    """
    –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î —Å–∏—Ä—ñ –¥–∞–Ω—ñ –∑ —Ñ–∞–π–ª—É Parquet.
    :param file_name: –ù–∞–∑–≤–∞ —Ñ–∞–π–ª—É –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è.
    :return: DataFrame —ñ–∑ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∏–º–∏ –¥–∞–Ω–∏–º–∏.
    """
    raw_data_path = join_path([RAW_PATH, f"{file_name}.parquet"])
    df = pd.read_parquet(raw_data_path, engine="pyarrow", columns=columns)
    logger.info(f"–î–∞–Ω—ñ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ –∑ {raw_data_path}")
    return df

def save_to_hdf5(data: dict, file_name: str):
    """
    –ó–±–µ—Ä—ñ–≥–∞—î –¥–∞–Ω—ñ —É —Ñ–æ—Ä–º–∞—Ç HDF5.
    :param data: –°–ª–æ–≤–Ω–∏–∫ —ñ–∑ –¥–∞–Ω–∏–º–∏ (–Ω–∞–∑–≤–∞ –Ω–∞–±–æ—Ä—É -> –º–∞—Å–∏–≤/—Å–ø–∏—Å–æ–∫/–¥–∞–Ω—ñ).
    :param file_path: –®–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É.
    """
    file_path = join_path([RAW_PATH, f"{file_name}.hdf5"])

    with h5py.File(file_path, "w") as f:
        for key, value in data.items():
            f.create_dataset(key, data=value)
    logger.info(f"–î–∞–Ω—ñ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É HDF5: {file_path}")

def read_from_hdf5(file_name: str) -> dict:
    """
    –ó—á–∏—Ç—É—î –¥–∞–Ω—ñ –∑ HDF5 —É —Å–ª–æ–≤–Ω–∏–∫.
    :param file_path: –®–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É.
    :return: –°–ª–æ–≤–Ω–∏–∫ —ñ–∑ –¥–∞–Ω–∏–º–∏ (–Ω–∞–∑–≤–∞ –Ω–∞–±–æ—Ä—É -> –¥–∞–Ω—ñ).
    """
    data = {}
    file_path = join_path([RAW_PATH, f"{file_name}.hdf5"])
    with h5py.File(file_path, "r") as f:
        for key in f.keys():
            data[key] = f[key][:]
    print(f"–î–∞–Ω—ñ –∑—á–∏—Ç–∞–Ω–æ –∑ HDF5: {file_path}")
    return data


def save_graphs(process_graph, path):
    """
    –ó–±–µ—Ä—ñ–≥–∞—î –≥—Ä–∞—Ñ–∏ —É –≤–∏–≥–ª—è–¥—ñ —Ñ–∞–π–ª—ñ–≤ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—É.
    """

    for node, attrs in process_graph.nodes(data=True):

        if attrs.get("type") == "process":
            graph_file = join_path([path, f"{node}.graphml"])
            subgraph = nx.ego_graph(process_graph, node)
            nx.write_graphml(subgraph, graph_file)
            logger.info(f"–ì—Ä–∞—Ñ –ø—Ä–æ—Ü–µ—Å—É {node} –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É {graph_file}.")


def save_graph(graph: nx.DiGraph, file_name: str, path: str = None):
    """
    –ó–±–µ—Ä—ñ–≥–∞—î –≥—Ä–∞—Ñ —É —Ñ–æ—Ä–º–∞—Ç—ñ GraphML.
    :param graph: –ì—Ä–∞—Ñ NetworkX.
    :param file_name: –ù–∞–∑–≤–∞ —Ñ–∞–π–ª—É –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è.
    :param path: –®–ª—è—Ö –¥–æ –ø–∞–ø–∫–∏ –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è.
    """
    try:
        if path:
            make_dir(path)  # –°—Ç–≤–æ—Ä—é—î –ø–∞–ø–∫—É, —è–∫—â–æ —ó—ó –Ω–µ —ñ—Å–Ω—É—î
            file_path = join_path([path, f"{file_name}.graphml"])
        else:
            file_path = f"{file_name}.graphml"  # –í–≤–∞–∂–∞—î–º–æ, —â–æ file_name –º—ñ—Å—Ç–∏—Ç—å –ø–æ–≤–Ω–∏–π —à–ª—è—Ö

        #file_path = join_path([path, f"{file_name}.graphml"])
        nx.write_graphml(graph, file_path)
        logger.debug(f"–ì—Ä–∞—Ñ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É {file_path}")

    except Exception as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è –≥—Ä–∞—Ñ–∞ {file_name}: {e}")
        raise


def load_graph(file_name: str, path: str = None) -> nx.DiGraph:
    """
    –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –≥—Ä–∞—Ñ —É —Ñ–æ—Ä–º–∞—Ç—ñ GraphML.

    :param file_name: –ù–∞–∑–≤–∞ —Ñ–∞–π–ª—É –≥—Ä–∞—Ñ–∞ –±–µ–∑ —Ä–æ–∑—à–∏—Ä–µ–Ω–Ω—è –∞–±–æ –ø–æ–≤–Ω–∏–π —à–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É.
    :param path: (–ù–µ–æ–±–æ–≤'—è–∑–∫–æ–≤–æ) –®–ª—è—Ö –¥–æ –ø–∞–ø–∫–∏, –¥–µ –∑–±–µ—Ä—ñ–≥–∞—î—Ç—å—Å—è —Ñ–∞–π–ª.
    :return: –ì—Ä–∞—Ñ NetworkX —É –≤–∏–≥–ª—è–¥—ñ nx.DiGraph.
    """
    try:
        if path:
            file_path = join_path([path, f"{file_name}.graphml"])
        else:
            file_path = f"{file_name}.graphml"  # –í–≤–∞–∂–∞—î–º–æ, —â–æ file_name –º—ñ—Å—Ç–∏—Ç—å –ø–æ–≤–Ω–∏–π —à–ª—è—Ö

        graph = nx.read_graphml(file_path)
        logger.debug(f"–ì—Ä–∞—Ñ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ –∑ {file_path}")
        return graph
    except FileNotFoundError:
        logger.error(f"–§–∞–π–ª –≥—Ä–∞—Ñ–∞ {file_name}.graphml –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –≤ {path or '–≤–∫–∞–∑–∞–Ω–æ–º—É —à–ª—è—Ö—É'}.")
        raise
    except Exception as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –≥—Ä–∞—Ñ–∞ {file_name}: {e}")
        raise


def save_graph_pic(graph: nx.DiGraph, file_name: str, path: str, visualize_func):
    """
    –ó–±–µ—Ä—ñ–≥–∞—î –≥—Ä–∞—Ñ —É –≤–∏–≥–ª—è–¥—ñ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é –ø–µ—Ä–µ–¥–∞–Ω–æ—ó —Ñ—É–Ω–∫—Ü—ñ—ó –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó.

    :param graph: –ì—Ä–∞—Ñ NetworkX.
    :param file_name: –ù–∞–∑–≤–∞ —Ñ–∞–π–ª—É –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è.
    :param path: –®–ª—è—Ö –¥–æ –ø–∞–ø–∫–∏ –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è.
    :param visualize_func: –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó –≥—Ä–∞—Ñ–∞.
    """
    try:
        make_dir(path)  # –°—Ç–≤–æ—Ä—é—î –ø–∞–ø–∫—É, —è–∫—â–æ —ó—ó –Ω–µ —ñ—Å–Ω—É—î
        file_path = join_path([path, f"{file_name}.png"])

        # –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —Ñ—É–Ω–∫—Ü—ñ—ó –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó
        visualize_func(graph, file_path)

        print(f"–ì—Ä–∞—Ñ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É {file_path}")
    except PermissionError as e:
        print(f"–ü–æ–º–∏–ª–∫–∞ –¥–æ—Å—Ç—É–ø—É –¥–æ {path}: {e}")
    except Exception as e:
        print(f"–ù–µ–≤—ñ–¥–æ–º–∞ –ø–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—ñ –≥—Ä–∞—Ñ–∞ {file_name}: {e}")

def initialize_register(file_name: str, columns:['id']):
    """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î —Ä–µ—î—Å—Ç—Ä, —è–∫—â–æ –π–æ–≥–æ —â–µ –Ω–µ —ñ—Å–Ω—É—î."""
    file_path = join_path([REGISTER_PATH, f"{file_name}.parquet"])
    if not is_file_exist(file_path):
        df = pd.DataFrame(columns=columns)
        df.to_parquet(file_path, index=False)

def save_register(df: pd.DataFrame, file_name: str):
    """–ó–±–µ—Ä—ñ–≥–∞—î —Ä–µ—î—Å—Ç—Ä."""
    reg_data_path = join_path([REGISTER_PATH, f"{file_name}.parquet"])
    df.to_parquet(reg_data_path, engine="pyarrow", index=False)
    logger.info(f"–î–∞–Ω—ñ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É {reg_data_path}")


def load_register(file_name: str, columns=None) -> pd.DataFrame:
    """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î —Ä–µ—î—Å—Ç—Ä."""
    reg_data_path = join_path([REGISTER_PATH, f"{file_name}.parquet"])
    df = pd.read_parquet(reg_data_path, engine="pyarrow", columns=columns)
    logger.info(f"–î–∞–Ω—ñ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ –∑ {reg_data_path}")
    return df

def save_checkpoint(model, optimizer, epoch, loss, file_path, stats=None):
    """
    –ó–±–µ—Ä—ñ–≥–∞—î —Å—Ç–∞–Ω –º–æ–¥–µ–ª—ñ, –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä–∞ —Ç–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –Ω–∞–≤—á–∞–Ω–Ω—è.

    :param model: PyTorch –º–æ–¥–µ–ª—å.
    :param optimizer: –û–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä –º–æ–¥–µ–ª—ñ.
    :param epoch: –ü–æ—Ç–æ—á–Ω–∞ –µ–ø–æ—Ö–∞ –Ω–∞–≤—á–∞–Ω–Ω—è.
    :param loss: –ó–Ω–∞—á–µ–Ω–Ω—è –ø–æ—Ç–æ—á–Ω–æ–≥–æ loss.
    :param file_path: –®–ª—è—Ö –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ñ–∞–π–ª—É.
    """
    checkpoint = {
        "model_state_dict": model.state_dict(),
        "optimizer_state_dict": optimizer.state_dict() if optimizer else None,
        "epoch": epoch,
        "loss": loss,
        "stats": stats if stats else {},  # –î–æ–¥–∞—î–º–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É, —è–∫—â–æ –≤–æ–Ω–∞ –ø–µ—Ä–µ–¥–∞–Ω–∞
    }
    torch.save(checkpoint, file_path)
    logger.info(f"–ß–µ–∫–ø–æ—ñ–Ω—Ç –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É {file_path}.")


def load_checkpoint(file_path, model, optimizer=None, stats=None):
    """
    –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î —Å—Ç–∞–Ω –º–æ–¥–µ–ª—ñ —Ç–∞ (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ) –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä–∞.

    :param file_path: –®–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è.
    :param model: PyTorch –º–æ–¥–µ–ª—å –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Å—Ç–∞–Ω—É.
    :param optimizer: (–ù–µ–æ–±–æ–≤'—è–∑–∫–æ–≤–æ) –û–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Å—Ç–∞–Ω—É.
    :return: epoch, loss (—è–∫—â–æ –≤–æ–Ω–∏ –∑–±–µ—Ä–µ–∂–µ–Ω—ñ).
    """
    if not is_file_exist(file_path):
        raise FileNotFoundError(f"–§–∞–π–ª —á–µ–∫–ø–æ—ñ–Ω—Ç–∞ –Ω–µ —ñ—Å–Ω—É—î –∑–∞ —à–ª—è—Ö–æ–º: {file_path}")

    try:
        checkpoint = torch.load(file_path)
        model.load_state_dict(checkpoint['model_state_dict'])
        if optimizer:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            print("Optimizer –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ")
        if stats:
            stats = checkpoint.get('stats')
            print(f"–°—Ç–∞—Ç–∏—Å–∏—Ç–∫—É –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ")

        epoch = checkpoint.get('epoch', 0)
        loss = checkpoint.get('loss', None)

        print(f"–ß–µ–∫–ø–æ—ñ–Ω—Ç –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ –∑ {file_path}")
        return epoch, loss, stats
    except Exception as e:
        raise RuntimeError(f"–ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —á–µ–∫–ø–æ—ñ–Ω—Ç–∞: {file_path}. –î–µ—Ç–∞–ª—ñ: {str(e)}")

def save_training_progress(register_name, progress_data):
    """
    –ó–±–µ—Ä—ñ–≥–∞—î –ø—Ä–æ–≥—Ä–µ—Å –Ω–∞–≤—á–∞–Ω–Ω—è —É —Ä–µ—î—Å—Ç—Ä.
    :param register_name: –ù–∞–∑–≤–∞ —Ä–µ—î—Å—Ç—Ä—É.
    :param progress_data: –î–∞–Ω—ñ –ø—Ä–æ–≥—Ä–µ—Å—É.
    """
    register = load_register(register_name)
    register = register.append(progress_data, ignore_index=True)
    save_register(register, register_name)
    logger.info(f"–ü—Ä–æ–≥—Ä–µ—Å –Ω–∞–≤—á–∞–Ω–Ω—è –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É —Ä–µ—î—Å—Ç—Ä {register_name}.")

def save_prepared_data(data, input_dim, doc_dim, global_node_dict, file_path):
    """
    –ó–±–µ—Ä—ñ–≥–∞—î –ø—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ñ –¥–∞–Ω—ñ —É —Ñ–∞–π–ª.
    :param data_list: –°–ø–∏—Å–æ–∫ –ø—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–∏—Ö –æ–±'—î–∫—Ç—ñ–≤ Data.
    :param input_dim: –í—Ö—ñ–¥–Ω–∏–π —Ä–æ–∑–º—ñ—Ä –¥–ª—è –º–æ–¥–µ–ª—ñ.
    :param file_path: –®–ª—è—Ö –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ñ–∞–π–ª—É.
    """
    torch.save({"data": data, "input_dim": input_dim, "doc_dim": doc_dim, "global_node_dict": global_node_dict}, file_path)
    print(f"–ü—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ñ –¥–∞–Ω—ñ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É {file_path}")

def load_prepared_data(file_path):
    """
    –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –ø—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ñ –¥–∞–Ω—ñ –∑ —Ñ–∞–π–ª—É.
    :param file_path: –®–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É.
    :return: data_list, input_dim.
    """
    try:
        checkpoint = torch.load(f"{file_path}")
        print(f"–ü—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ñ –¥–∞–Ω—ñ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ –∑ {file_path}")
        return checkpoint["data"], checkpoint["input_dim"], checkpoint["doc_dim"], checkpoint["global_node_dict"]
    except FileNotFoundError:
        print(f"–§–∞–π–ª –∑ –ø—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–∏–º–∏ –¥–∞–Ω–∏–º–∏ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {file_path}")
        return None, None, None, None

def save_statistics_to_json(stats, file_path):
    """
    –ó–±–µ—Ä—ñ–≥–∞—î —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤ JSON —Ñ–∞–π–ª.

    :param stats: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è.
    :param file_path: –®–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É.
    """

    def convert_to_serializable(obj):
        if isinstance(obj, (np.int64, np.int32)):
            return int(obj)
        elif isinstance(obj, (np.float64, np.float32)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        raise TypeError(f"Object of type {type(obj)} is not JSON serializable")

    with open(f"{file_path}.json", "w") as f:
        json.dump(stats, f, indent=4, default=convert_to_serializable)


def load_global_statistics_from_json(file_path):
    """
    –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –≥–ª–æ–±–∞–ª—å–Ω—É —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∑ JSON-—Ñ–∞–π–ª—É.

    :param file_path: –®–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É (–±–µ–∑ .json)
    :return: –°–ª–æ–≤–Ω–∏–∫ –∑—ñ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ—é
    """
    with open(f"{file_path}.json", "r") as f:
        stats = json.load(f)
    return stats

def save2csv(df: pd.DataFrame, file_name: str):
    """–ó–±–µ—Ä—ñ–≥–∞—î —Ä–µ—î—Å—Ç—Ä."""
    #print(df)
    if not isinstance(df, pd.DataFrame):
        if isinstance(df, dict) and all(isinstance(v, list) for v in df.values()):
            # –í–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è —Å–ø–∏—Å–∫—ñ–≤ –∑–∞ –¥–æ–≤–∂–∏–Ω–æ—é
            max_length = max(len(values) for values in df.values())
            for key, values in df.items():
                if len(values) < max_length:
                    df[key] = values + [None] * (max_length - len(values))
            df = pd.DataFrame(df)
        else:
            logger.error("–ù–µ–ø—ñ–¥—Ç—Ä–∏–º—É–≤–∞–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–∏—Ö –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è.")
            return

    try:
        reg_data_path = f"{file_name}.xlsx"
        df.to_excel(reg_data_path, index=False)
        logger.info(f"–î–∞–Ω—ñ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É {reg_data_path}")
    except Exception as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—ñ –¥–∞–Ω–∏—Ö: {e}")


def save_confusion_matrix_to_csv(cm, class_labels, file_path):
    """
    –ó–±–µ—Ä—ñ–≥–∞—î confusion matrix —É CSV/XLSX –∑ –ø—ñ–¥–ø–∏—Å–∞–º–∏ –∫–ª–∞—Å—ñ–≤ –ø–æ –æ—Å—è—Ö.

    :param cm: numpy.ndarray, –º–∞—Ç—Ä–∏—Ü—è –ø–ª—É—Ç–∞–Ω–∏–Ω–∏.
    :param class_labels: —Å–ø–∏—Å–æ–∫ —ñ–¥–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ç–æ—Ä—ñ–≤ –∫–ª–∞—Å—ñ–≤ (–≤—É–∑–ª—ñ–≤).
    :param file_path: —à–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É –±–µ–∑ —Ä–æ–∑—à–∏—Ä–µ–Ω–Ω—è (–±—É–¥–µ .xlsx).
    """
    df_cm = pd.DataFrame(cm, index=class_labels, columns=class_labels)
    df_cm.index.name = 'True Label'
    df_cm.columns.name = 'Predicted Label'
    df_cm.to_excel(f"{file_path}.xlsx")